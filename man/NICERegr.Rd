% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/NICERegr.R
\name{NICERegr}
\alias{NICERegr}
\title{NICE (Nearest Instance Counterfactual Explanations) for Regression Tasks}
\description{
NICE (Brughmans and Martens 2021) searches for counterfactuals by iteratively replacing feature values
of \code{x_interest} with the corresponding value of its most similar (optionally correctly predicted) instance \code{x_nn}.
While the original method is only applicable to classification tasks (see \link{NICEClassif}), this implementation extend it to regression tasks.
}
\details{
NICE starts the counterfactual search for \code{x_interest} by finding its most similar (optionally) correctly predicted
neighbor \code{x_nn} with(in) the desired prediction (range). Correctly predicted means that the prediction of \code{x_nn} is less
than a user-specified \code{margin_correct_classif} away from the true outcome of \code{x_nn}.

\cr
In the first iteration, NICE creates new instances by replacing a different feature value of \code{x_interest} with the corresponding
value of \code{x_nn} in each new instance. Thus, if \code{x_nn} differs from \code{x_interest} in \code{d} features, \code{d} new instances are created. \cr
Then, the reward values for the created instances are computed with the chosen reward function.
Available reward functions are \code{sparsity}, \code{proximity}, and \code{plausibility}. \cr
In the second iteration, NICE creates \code{d-1} new instances by replacing a different feature value of the highest
reward instance of the previous iteration with the corresponding value of \code{x_interest}, and so on. \cr
If \code{finish_early = TRUE}, the algorithm terminates when the predicted outcome for
the highest reward instance is in the interval \code{desired_outcome}; if \code{finish_early = FALSE}, the
algorithm continues until \code{x_nn} is recreated. \cr
Once the algorithm terminated, it depends on \code{return_multiple} which instances
are returned as counterfactuals: if \code{return_multiple = FALSE}, then only the highest reward instance in the
last iteration is returned as counterfactual; if \code{return_multiple = TRUE}, then all instances (of all iterations)
whose predicted outcome is in the interval \code{desired_outcome} are returned as counterfactuals.

If \code{finish_early = FALSE} and \code{return_multiple = FALSE}, then \code{x_nn} is returned as single counterfactual.

The function computes the dissimilarities using Gower's dissimilarity measure (Gower 1971).
}
\examples{
\dontrun{
if (require("randomForest")) {
  set.seed(123456)
  # Train a model
  rf = randomForest(mpg ~ ., data = mtcars)
  # Create a predictor object
  predictor = iml::Predictor$new(rf)
  # Find counterfactuals
  nice_regr = NICERegr$new(predictor)
  cfactuals = nice_regr$find_counterfactuals(
     x_interest = mtcars[1L, ], desired_outcome = c(22, 26)
  )
  # Print the results
  cfactuals$data
  # Print archive
  nice_regr$archive
}
}

}
\references{
Brughmans, D., & Martens, D. (2021). NICE: An Algorithm for Nearest Instance Counterfactual Explanations.
arXiv preprint arXiv:2104.07411.

Gower, J. C. (1971), "A general coefficient of similarity and some of its properties". Biometrics, 27, 623â€“637.
}
\section{Super classes}{
\code{\link[counterfactuals:CounterfactualMethod]{counterfactuals::CounterfactualMethod}} -> \code{\link[counterfactuals:CounterfactualMethodRegr]{counterfactuals::CounterfactualMethodRegr}} -> \code{NICEClassif}
}
\section{Active bindings}{
\if{html}{\out{<div class="r6-active-bindings">}}
\describe{
\item{\code{x_nn}}{(\code{logical(1)}) \cr
The most similar (optionally) correctly classified instance of \code{x_interest}.}

\item{\code{archive}}{(\code{list()}) \cr
A list that stores the history of the algorithm run. For each algorithm iteration, it has one element containing
a \code{data.table}, which stores all created instances of this iteration together with their
reward values and their predictions.}
}
\if{html}{\out{</div>}}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-new}{\code{NICERegr$new()}}
\item \href{#method-clone}{\code{NICERegr$clone()}}
}
}
\if{html}{
\out{<details open ><summary>Inherited methods</summary>}
\itemize{
\item \out{<span class="pkg-link" data-pkg="counterfactuals" data-topic="CounterfactualMethod" data-id="print">}\href{../../counterfactuals/html/CounterfactualMethod.html#method-print}{\code{counterfactuals::CounterfactualMethod$print()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="counterfactuals" data-topic="CounterfactualMethodRegr" data-id="find_counterfactuals">}\href{../../counterfactuals/html/CounterfactualMethodRegr.html#method-find_counterfactuals}{\code{counterfactuals::CounterfactualMethodRegr$find_counterfactuals()}}\out{</span>}
}
\out{</details>}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-new"></a>}}
\if{latex}{\out{\hypertarget{method-new}{}}}
\subsection{Method \code{new()}}{
Create a new NICERegr object.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{NICERegr$new(
  predictor,
  optimization = "sparsity",
  x_nn_correct_classif = TRUE,
  margin_correct_classif = NULL,
  return_multiple = TRUE,
  finish_early = TRUE,
  distance_function = NULL
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{predictor}}{(\link[iml]{Predictor})\cr
The object (created with \code{iml::Predictor$new()}) holding the machine learning model and the data.}

\item{\code{optimization}}{(\code{character(1)})\cr
The reward function to optimize. Can be \code{sparsity} (default), \code{proximity} or \code{plausibility}.}

\item{\code{x_nn_correct_classif}}{(\code{logical(1)})\cr
Should only \emph{correctly} classified data points in \code{predictor$data$X} be considered for the most similar instance search?
Default is \code{TRUE}.}

\item{\code{margin_correct_classif}}{(\code{numeric(1)} | \code{NULL})\cr
The accepted margin for considering a prediction as "correct". As the initial version of NICE was designed for
classification tasks only, this is a design to mimic the search for \code{x_nn} for regression tasks.
Ignored if \code{x_nn_correct_classif = FALSE}.
TODO: default}

\item{\code{return_multiple}}{(\code{logical(1)})\cr
Should multiple counterfactuals be returned? If TRUE, the algorithm returns all created instances whose
prediction is in the interval \code{desired_outcome}. For more information, see the \code{Details} section.}

\item{\code{finish_early}}{(\code{logical(1)})\cr

Should the algorithm terminate after an iteration in which the \code{desired_class} prediction for the highest reward instance
is in the interval \code{desired_prob}. If \code{FALSE}, the algorithm continues until \code{x_nn} is recreated.}

\item{\code{distance_function}}{(\verb{function()} | \code{NULL})\cr
The distance function used to compute the distances between \code{x_interest} and the training data points for finding \code{x_nn}.
The function must have three arguments: \code{x}, \code{y}, and \code{data} and return a \code{double} matrix with \code{nrow(x)} rows
and \code{nrow(y)} columns. If set to \code{NULL} (default), then Gower distance (Gower 1971) is used.}

}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-clone"></a>}}
\if{latex}{\out{\hypertarget{method-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{NICERegr$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
